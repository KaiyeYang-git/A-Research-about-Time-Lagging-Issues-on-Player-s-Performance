{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\"\"\"实习题目2：\r\n",
    "\r\n",
    "题目描述\r\n",
    "对时间序列进行异常点检测：使用提供的train（长度18天）和test（长度6天）数据，设计异常检测模型，模型 类型不限，可以为有监督或无监督，对test曲线的剩余部分（14天）进行异常检测，并按数据样例的格式输出检测 结果\r\n",
    "\r\n",
    "评价方式\r\n",
    "precision: 若检测到t时刻为异常，寻找距离t最近的实际异常点t1，score = (5-distance(t,t1)) / 5，score小于0则 记为0\r\n",
    "统计所有检测到的异常点的得分，求均值做为最终precision得分\r\n",
    "\r\n",
    "recall: 若t1时刻为实际异常点，寻找距离t最近的检测到的异常点t，score = (5-distance(t,t1)) / 5，score小 于0则记为0\r\n",
    "统计所有实际异常点的得分，求均值做为最终recall得分 3.f1\r\n",
    "\r\n",
    "f1 = (1+1.2**2) * P * R / (P*1.2**2 + R)\r\n",
    "\r\n",
    "数据说明\r\n",
    "1.train 训练数据包含100个文件，每个文件中是一条长度19天的曲线数据，时间频率为1分钟，每条曲线数据彼此独立 数据有异常标签（仅供参考，可能有噪声）\r\n",
    "2.test\r\n",
    "验证数据包含37个文件，每个文件中是一条长度6天的曲线数据，时间频率为1分钟，每条曲线数据彼此独立 均无标签\r\n",
    "验证数据的后14天数据，将在验收时提供\r\n",
    "\r\n",
    "数据样例\r\n",
    "value,time,alert_flag 171.15613,2020-05-19 00:00:00,-1 171.47934,2020-05-19 00:01:00,1 171.74709,2020-05-19 00:02:00,1\r\n",
    "value：采样数值 time：采样时间 alert_flag：异常标志，0为正常，1为异常，-1为未知\r\n",
    "\r\n",
    "为了方便起见\r\n",
    "\r\n",
    "value由三个数值分段产生，将为string\r\n",
    "time由年份月份和日期产生，将为datatime或者string\r\n",
    "alert_flag由0，1组成，将为boolearn\r\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'实习题目2：\\n\\n题目描述\\n对时间序列进行异常点检测：使用提供的train（长度18天）和test（长度6天）数据，设计异常检测模型，模型 类型不限，可以为有监督或无监督，对test曲线的剩余部分（14天）进行异常检测，并按数据样例的格式输出检测 结果\\n\\n评价方式\\nprecision: 若检测到t时刻为异常，寻找距离t最近的实际异常点t1，score = (5-distance(t,t1)) / 5，score小于0则 记为0\\n统计所有检测到的异常点的得分，求均值做为最终precision得分\\n\\nrecall: 若t1时刻为实际异常点，寻找距离t最近的检测到的异常点t，score = (5-distance(t,t1)) / 5，score小 于0则记为0\\n统计所有实际异常点的得分，求均值做为最终recall得分 3.f1\\n\\nf1 = (1+1.2**2) * P * R / (P*1.2**2 + R)\\n\\n数据说明\\n1.train 训练数据包含100个文件，每个文件中是一条长度19天的曲线数据，时间频率为1分钟，每条曲线数据彼此独立 数据有异常标签（仅供参考，可能有噪声）\\n2.test\\n验证数据包含37个文件，每个文件中是一条长度6天的曲线数据，时间频率为1分钟，每条曲线数据彼此独立 均无标签\\n验证数据的后14天数据，将在验收时提供\\n\\n数据样例\\nvalue,time,alert_flag 171.15613,2020-05-19 00:00:00,-1 171.47934,2020-05-19 00:01:00,1 171.74709,2020-05-19 00:02:00,1\\nvalue：采样数值 time：采样时间 alert_flag：异常标志，0为正常，1为异常，-1为未知\\n\\n为了方便起见\\n\\nvalue由三个数值分段产生，将为string\\ntime由年份月份和日期产生，将为datatime或者string\\nalert_flag由0，1组成，将为boolearn\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T03:57:18.414281Z",
     "iopub.status.busy": "2021-09-02T03:57:18.414084Z",
     "iopub.status.idle": "2021-09-02T03:57:18.423638Z",
     "shell.execute_reply": "2021-09-02T03:57:18.423145Z",
     "shell.execute_reply.started": "2021-09-02T03:57:18.414257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import os, datetime as dt, math\r\n",
    "import datetime\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import plotly.express as px\r\n",
    "import featuretools as ft\r\n",
    "import sklearn\r\n",
    "\r\n",
    "#寻找到主路径，将主路径所有数据包（csv格式）解析过滤,排除文件中的隐藏成分\r\n",
    "#再通过路径合并，读取文件夹中每一个文件\r\n",
    "\r\n",
    "path = r'D:/Computer Science/Fault-Analysis/train/'\r\n",
    "#path = r'/data/jupyterhub/notebook/dickensyang_jupyter/Fault_Analysis/train/'\r\n",
    "files = list(filter(lambda x: x!='.ipynb_checkpoints', os.listdir(path)))\r\n",
    "allTrainData=[]\r\n",
    "for data in files:\r\n",
    "    readAllCsv = pd.read_csv(path+data, index_col=None, header=0, sep=\",\")\r\n",
    "    allTrainData.append(readAllCsv)\r\n",
    "allData=pd.concat(allTrainData,ignore_index=True)\r\n",
    "\r\n",
    "#def features(allData):\r\n",
    "#特征一览\r\n",
    "#特征1：时间序列\r\n",
    "time=pd.to_datetime(allData['time'], format='%Y-%m-%d %H:%M:%S')\r\n",
    "\r\n",
    "#特征2：两点之间的差值\r\n",
    "valueDiff = (allData['value']-allData['value'].shift(periods=1)).dropna(axis=0, how='any')\r\n",
    "valueDiffMean = pd.Series(valueDiff.mean())\r\n",
    "valueDiff = valueDiffMean.append(valueDiff, ignore_index=True)\r\n",
    "#特征3:两点之间的距离 (现在把所有时间序列间隔定义为 1)\r\n",
    "allData['index'] = allData.index\r\n",
    "indexDiff = (allData['index']-allData['index'].shift(periods=1)).dropna(axis=0, how='any')\r\n",
    "indexDiffMean = pd.Series(indexDiff.mean())\r\n",
    "indexDiff = indexDiffMean.append(indexDiff, ignore_index=True)\r\n",
    "disDiff=(valueDiff**2+indexDiff**2).apply(math.sqrt)\r\n",
    "#特征5：两点之间斜率\r\n",
    "#需要把时间参数转化为可以计算的       \r\n",
    "slope=valueDiff/indexDiff\r\n",
    "#特征6:两点之间的正弦值和余弦值\r\n",
    "sin = valueDiff/(valueDiff**2+indexDiff**2).apply(math.sqrt)\r\n",
    "cos = indexDiff/(valueDiff**2+indexDiff**2).apply(math.sqrt)\r\n",
    "#特征7:两点之间算术平均值和几何平均值\r\n",
    "add = ((allData['value']+allData['value'].shift(periods=1))/2).dropna(axis=0, how='any')\r\n",
    "addMean = pd.Series(add.mean())\r\n",
    "add = addMean.append(add,ignore_index=True)\r\n",
    "multi=((allData['value']*allData['value'].shift(periods=1)).apply(math.sqrt)).dropna(axis=0, how='any')\r\n",
    "multiMean = pd.Series(multi.mean())\r\n",
    "multi = multiMean.append(multi, ignore_index=True)\r\n",
    "\r\n",
    "#特征8：两点之间同比/3天环比/7天环比\r\n",
    "dayTodayGap=pd.DataFrame(allData['value'].shift(periods=1440).dropna(axis=0, how='any'))\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>185.81683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>185.41160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>185.35612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>185.13422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>184.56013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51698</th>\n",
       "      <td>2204.23140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51699</th>\n",
       "      <td>2205.27300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51700</th>\n",
       "      <td>2203.71070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51701</th>\n",
       "      <td>2211.00120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51702</th>\n",
       "      <td>2210.48050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            value\n",
       "1440    185.81683\n",
       "1441    185.41160\n",
       "1442    185.35612\n",
       "1443    185.13422\n",
       "1444    184.56013\n",
       "...           ...\n",
       "51698  2204.23140\n",
       "51699  2205.27300\n",
       "51700  2203.71070\n",
       "51701  2211.00120\n",
       "51702  2210.48050\n",
       "\n",
       "[50263 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T12:12:50.742593Z",
     "iopub.status.busy": "2021-09-02T12:12:50.742316Z",
     "iopub.status.idle": "2021-09-02T12:13:48.775570Z",
     "shell.execute_reply": "2021-09-02T12:13:48.774439Z",
     "shell.execute_reply.started": "2021-09-02T12:12:50.742565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#for allData in allTrainData:\r\n",
    "#features(allData)\r\n",
    "\r\n",
    "# dayToday = (allData['value']-\r\n",
    "#             )/allData['value'].shift(periods=1440).dropna(axis=0, how='any')\r\n",
    "# dayTodayMean = pd.Series(dayToday.mean())\r\n",
    "# dayToday = dayTodayMean.append(dayToday, ignore_index=True)\r\n",
    "# print(dayToday)    \r\n",
    "#threedaytoday=all_data['value']/all_data['value'].shift(periods=4320).dropna(axis=0, how='any')\r\n",
    "#sevendaytoday=all_data['value']/all_data['value'].shift(periods=10080).dropna(axis=0, how='any')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#特征8：时间准点标记\r\n",
    "    timeList=list(range(1,24))\r\n",
    "    timeListDict=[]\r\n",
    "    for H in timeList:\r\n",
    "        if H<10:\r\n",
    "            H='0'+ str(H)\r\n",
    "        else:\r\n",
    "            H=H\r\n",
    "        timeListDict.append(int(H))\r\n",
    "\r\n",
    "    timeListDict2=timeListDict[1:23]\r\n",
    "\r\n",
    "    for a in timeListDict:\r\n",
    "        startTime1=dt.datetime.strptime(f'{a}:50:00', '%H:%M:%S').time()\r\n",
    "        endTime1=dt.datetime.strptime(f'{a}:59:00','%H:%M:%S').time()\r\n",
    "        time_on_clock1=time[(start_time1<=time.dt.time) & (end_time1>=time.dt.time)] \r\n",
    "\r\n",
    "    for b in time_list_dict_n:\r\n",
    "        start_time2=dt.datetime.strptime(f'{b}:00:00', '%H:%M:%S').time()\r\n",
    "        end_time2=dt.datetime.strptime(f'{b}:10:00','%H:%M:%S').time()\r\n",
    "        time_on_clock2=time[(start_time2<=time.dt.time) & (end_time2>=time.dt.time)]\r\n",
    "\r\n",
    "    time_fix=(pd.concat([time_on_clock1,time_on_clock2],sort=True)).to_frame()\r\n",
    "\r\n",
    "    time_fix['index']=time_fix.index\r\n",
    "    all_data['index']=all_data.index\r\n",
    "\r\n",
    "    if time_fix['index'].eq(all_data['index']).any()==True:\r\n",
    "        new_alert_flag=all_data['alert_flag'].replace(to_replace=0,value=1)\r\n",
    "        new_alert_flag=all_data['alert_flag'].replace(to_replace=1,value=1)\r\n",
    "    else:\r\n",
    "        new_alert_flag=all_data['alert_flag'].replace(to_replace=0,value=0)\r\n",
    "        new_alert_flag=all_data['alert_flag'].replace(to_replace=1,value=0)\r\n",
    "\r\n",
    "#确认完特征后，由于每个特征在绝对值上差距过多，并在后涉及距离计算，因此将通过标准化处理每个特征\r\n",
    "    value_standard = (all_data['value']-all_data['value'].mean())/all_data['value'].std()\r\n",
    "    value_diff_standard=(value_diff-value_diff.mean())/value_diff.std()\r\n",
    "    dis_diff_standard=(dis_diff-dis_diff.mean())/dis_diff.std()\r\n",
    "    slope_standard=(slope-slope.mean())/slope.std()\r\n",
    "    sin_standard=(sin-sin.mean())/sin.std()\r\n",
    "    cos_standard=(cos-cos.mean())/cos.std()\r\n",
    "    add_standard=(add-add.mean())/add.std()\r\n",
    "    multi_standard=(multi-multi.mean())/multi.std()\r\n",
    "#     daytoday_standard=(daytoday-daytoday.mean())/daytoday.std()\r\n",
    "#     threedaytoday_standard=(threedaytoday-threedaytoday.mean())/threedaytoday.std()\r\n",
    "#     sevendaytoday_standard=(sevendaytoday-sevendaytoday.mean())/sevendaytoday.std()\r\n",
    "\r\n",
    "    dict={'time':time,\r\n",
    "          'value_standard':value_standard,\r\n",
    "          'value_diff_standard':value_diff_standard,\r\n",
    "          'dis_diff_standard':dis_diff_standard,\r\n",
    "          'slope_standard':slope_standard,\r\n",
    "          'sin_standard':sin_standard,\r\n",
    "          'cos_standard':cos_standard,\r\n",
    "          'add_standard':add_standard,\r\n",
    "          'multi_standard':multi_standard,\r\n",
    "#           'daytoday_standard':daytoday_standard,\r\n",
    "#           'threedaytoday_standard':threedaytoday_standard,\r\n",
    "#           'sevendaytoday_standard':sevendaytoday_standard,\r\n",
    "          'new_alert_flag':new_alert_flag,\r\n",
    "          'alert_flag':all_data['alert_flag']}\r\n",
    "\r\n",
    "    new_all_data=pd.DataFrame(dict)[1:2556630].dropna()\r\n",
    "    es=ft.EntitySet(id='feature_creation')\r\n",
    "    es=es.entity_from_dataframe(entity_id='new_all_data',dataframe=new_all_data[['value_standard','value_diff_standard','dis_diff_standard','slope_standard','sin_standard','cos_standard','add_standard','multi_standard']], index='index', make_index=True)\r\n",
    "    trans_primitives=['add_numeric', 'multiply_numeric', 'subtract_numeric']#'divide_numeric'  # 2列相加减乘除来生成新特征\r\n",
    "    # max_depth=50，在原特征上进行50次运算产生新特征\r\n",
    "    feature_matrix, feature_names = ft.dfs(entityset=es, target_entity='new_all_data', trans_primitives=trans_primitives)\r\n",
    "    feature_matrix['new_alert_flag']=new_alert_flag\r\n",
    "    #主成因分析法,使用mle算法\r\n",
    "    from sklearn.decomposition import PCA\r\n",
    "    pca=PCA(n_components='mle', svd_solver='full',random_state=10)\r\n",
    "    data_pca=pca.fit_transform(feature_matrix)\r\n",
    "    #通过PCA中mle模型发现个成分\r\n",
    "\r\n",
    "    #选取被强化的特征和最初的alert_flag进行训练集和测试集分类\r\n",
    "    from sklearn.model_selection import train_test_split\r\n",
    "    # 标注出所有测试特征和异常点分类\r\n",
    "    data=data_pca\r\n",
    "    label=new_all_data['alert_flag'].tolist()\r\n",
    "    \r\n",
    "    #X为所有的X值，y为X值所对应的结果值。test_size=0.20表示测试集占20%\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20,random_state=10)\r\n",
    "    \r\n",
    "    from sklearn.ensemble import RandomForestClassifier\r\n",
    "    from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "    randomfor=RandomForestClassifier(n_estimators=100,max_depth=10,min_samples_split=2, n_jobs=-1,random_state=10).fit(X_train,y_train)\r\n",
    "    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "    #X为所有的X值，y为X值所对应的结果值。test_size=0.20表示测试集占20%\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20,random_state=10)\r\n",
    "    \r\n",
    "    from sklearn.ensemble import RandomForestClassifier\r\n",
    "    from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "    randomfor=RandomForestClassifier(n_estimators=100,max_depth=10,min_samples_split=2, n_jobs=-1,random_state=10).fit(X_train,y_train)\r\n",
    "    return randomfor.score(X_train,y_train)\r\n",
    "\r\n",
    "for all_data in all_train_data:\r\n",
    "    features(all_data)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T11:10:59.177519Z",
     "iopub.status.busy": "2021-09-02T11:10:59.177259Z",
     "iopub.status.idle": "2021-09-02T11:11:23.460784Z",
     "shell.execute_reply": "2021-09-02T11:11:23.460153Z",
     "shell.execute_reply.started": "2021-09-02T11:10:59.177492Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# #网格测试，以确定最优参数（过于耗时，对于大体量数据不建议这么跑）\r\n",
    "# from sklearn.ensemble import RandomForestClassifier\r\n",
    "# from sklearn.model_selection import cross_val_score\r\n",
    "# from sklearn.model_selection import GridSearchCV\r\n",
    "# from sklearn import metrics\r\n",
    "# param_test1= {'n_estimators':range(100,500,50)}  \r\n",
    "# gsearch1 = GridSearchCV(estimator = RandomForestClassifier(max_depth=10,random_state=10), param_grid=param_test1,cv=5)  \r\n",
    "# gsearch1.fit(X_train,y_train)  \r\n",
    "# print(gsearch1.best_params_)\r\n",
    "\r\n",
    "# param_test2= {'max_depth':range(1,11,1),'min_samples_split':range(5,15,1)}  \r\n",
    "# gsearch2= GridSearchCV(estimator = RandomForestClassifier(n_estimators=100, random_state=10), param_grid = param_test2,scoring='roc_auc', cv=5)  \r\n",
    "# gsearch2.fit(X_train,y_train)  \r\n",
    "# print(gsearch2.best_params_) \r\n",
    "# # 最后确定为 100，10，5"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T04:02:22.226443Z",
     "iopub.status.busy": "2021-09-02T04:02:22.226244Z",
     "iopub.status.idle": "2021-09-02T04:02:22.229124Z",
     "shell.execute_reply": "2021-09-02T04:02:22.228618Z",
     "shell.execute_reply.started": "2021-09-02T04:02:22.226419Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    from sklearn.ensemble import RandomForestClassifier\r\n",
    "    from sklearn.model_selection import cross_val_score\r\n",
    "    from sklearn.model_selection import GridSearchCV\r\n",
    "    from sklearn import metrics\r\n",
    "\r\n",
    "    randomfor=RandomForestClassifier(n_estimators=100,max_depth=10,min_samples_split=2, n_jobs=-1,random_state=10).fit(X_train,y_train)\r\n",
    "    precision_randomfor=cross_val_score(randomfor, X_test, y_test, cv=5, scoring='precision')\r\n",
    "    recall_randomfor=cross_val_score(randomfor, X_test, y_test, cv=5, scoring='recall')\r\n",
    "    f1=np.mean(np.nan_to_num((1+1.2**2) * precision_randomfor * recall_randomfor / (precision_randomfor*1.2**2 + recall_randomfor)))\r\n",
    "    print(precision_randomfor,recall_randomfor,f1)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T06:34:02.982951Z",
     "iopub.status.busy": "2021-09-02T06:34:02.982680Z",
     "iopub.status.idle": "2021-09-02T06:42:36.823108Z",
     "shell.execute_reply": "2021-09-02T06:42:36.822480Z",
     "shell.execute_reply.started": "2021-09-02T06:34:02.982921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# XGB模型参数设置\r\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\r\n",
    "# xgboosting= xgb.XGBClassifier(n_estimators=100,subsample=1,colsample_bynode=1)\r\n",
    "# xgboosting_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\r\n",
    "# precision_scores = cross_val_score(xgboosting, X_test, y_test, scoring='precision', cv=xgboosting_cv, n_jobs=-1)\r\n",
    "# recall_scores = cross_val_score(xgboosting, X_test, y_test, scoring='recall', cv=xgboosting_cv, n_jobs=-1)\r\n",
    "# f1=np.mean(np.nan_to_num((1+1.2**2) * precision_scores * recall_scores / (precision_scores*1.2**2 + recall_scores)))\r\n",
    "# print(precision_scores,recall_scores,f1)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T04:39:13.658369Z",
     "iopub.status.idle": "2021-09-02T04:39:13.659067Z",
     "shell.execute_reply": "2021-09-02T04:39:13.658846Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-02T04:39:13.658369Z",
     "iopub.status.idle": "2021-09-02T04:39:13.659067Z",
     "shell.execute_reply": "2021-09-02T04:39:13.658846Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path_test = r'/data/jupyterhub/notebook/dickensyang_jupyter/Fault_Analysis/test/'\r\n",
    "files_test = list(filter(lambda x: x!='.ipynb_checkpoints', os.listdir(path_test)))\r\n",
    "\r\n",
    "lists_test=[]\r\n",
    "for data_test in files_test:\r\n",
    "    read_test_csv = pd.read_csv(path_test+data_test, index_col=None, header=0, sep=\",\")\r\n",
    "    lists_test.append(read_test_csv)\r\n",
    "\r\n",
    "data_test=pd.concat(lists_test,ignore_index=True)\r\n",
    "\r\n",
    "data_test['index']=data_test.index\r\n",
    "time_test=pd.to_datetime(data_test['time'], format='%Y-%m-%d %H:%M:%S')\r\n",
    "time_diff_test=((time_test-time_test.shift(periods=1)).dropna(axis=0, how='any')).apply(lambda x:x.total_seconds()/60)\r\n",
    "value_diff_test = (data_test['value']-data_test['value'].shift(periods=1)).dropna(axis=0, how='any')\r\n",
    "index_diff_test=(data_test['index']-data_test['index'].shift(periods=1)).dropna(axis=0, how='any')\r\n",
    "dis_diff_test=(value_diff_test**2+index_diff_test**2).apply(math.sqrt)\r\n",
    "slope_test=value_diff_test/index_diff_test\r\n",
    "sin_test=value_diff_test/(value_diff_test**2+index_diff_test**2).apply(math.sqrt)\r\n",
    "cos_test=index_diff_test/(value_diff_test**2+index_diff_test**2).apply(math.sqrt)\r\n",
    "add_test=((data_test['value']+data_test['value'].shift(periods=1))/2).dropna(axis=0, how='any')\r\n",
    "multi_test=((data_test['value']*data_test['value'].shift(periods=1)).apply(math.sqrt)).dropna(axis=0, how='any')\r\n",
    "\r\n",
    "time_list_test=list(range(1,24))\r\n",
    "time_list_test_dict=[]\r\n",
    "for M in time_list_test:\r\n",
    "    if M<10:\r\n",
    "        M='0'+ str(M)\r\n",
    "    else:\r\n",
    "        M=M\r\n",
    "    time_list_test_dict.append(int(M))\r\n",
    "time_list_test_dict_n=time_list_test_dict[1:23]\r\n",
    "\r\n",
    "for A in time_list_test_dict:\r\n",
    "    start_time_test_1=dt.datetime.strptime(f'{A}:50:00', '%H:%M:%S').time()\r\n",
    "    end_time_test_1=dt.datetime.strptime(f'{A}:59:00','%H:%M:%S').time()\r\n",
    "    time_on_clock_test_1=time_test[(start_time_test_1<=time_test.dt.time) & (end_time_test_1>=time_test.dt.time)] \r\n",
    "\r\n",
    "for B in time_list_test_dict_n:\r\n",
    "    start_time_test_2=dt.datetime.strptime(f'{B}:00:00', '%H:%M:%S').time()\r\n",
    "    end_time_test_2=dt.datetime.strptime(f'{B}:10:00','%H:%M:%S').time()\r\n",
    "    time_on_clock_test_2=time_test[(start_time_test_2<=time_test.dt.time) & (end_time_test_2>=time_test.dt.time)]\r\n",
    "\r\n",
    "time_test_fix=(pd.concat([time_on_clock_test_1,time_on_clock_test_2],sort=True)).to_frame()\r\n",
    "time_test_fix['index']=time_test_fix.index\r\n",
    "data_test['index']=data_test.index\r\n",
    "\r\n",
    "if time_test_fix['index'].eq(data_test['index']).any()==True:\r\n",
    "    new_test_alert_flag=data_test['alert_flag'].replace(to_replace=0,value=1)\r\n",
    "    new_test_alert_flag=data_test['alert_flag'].replace(to_replace=1,value=1)\r\n",
    "else:\r\n",
    "    new_test_alert_flag=data_test['alert_flag'].replace(to_replace=0,value=0)\r\n",
    "    new_test_alert_flag=data_test['alert_flag'].replace(to_replace=1,value=0)\r\n",
    "\r\n",
    "time_diff_test_standard=(time_diff_test-time_diff_test.mean())/time_diff_test.std()\r\n",
    "value_test_standard = (data_test['value']-data_test['value'].mean())/data_test['value'].std()\r\n",
    "value_diff_test_standard=(value_diff_test-value_diff_test.mean())/value_diff_test.std()\r\n",
    "dis_diff_test_standard=(dis_diff_test-dis_diff_test.mean())/dis_diff_test.std()\r\n",
    "slope_test_standard=(slope_test-slope_test.mean())/slope_test.std()\r\n",
    "sin_test_standard=(sin_test-sin_test.mean())/sin_test.std()\r\n",
    "cos_test_standard=(cos_test-cos_test.mean())/cos_test.std()\r\n",
    "add_test_standard=(add_test-add_test.mean())/add_test.std()\r\n",
    "multi_test_standard=(multi_test-multi_test.mean())/multi_test.std()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T06:23:10.746036Z",
     "iopub.status.busy": "2021-09-02T06:23:10.745776Z",
     "iopub.status.idle": "2021-09-02T06:25:05.041174Z",
     "shell.execute_reply": "2021-09-02T06:25:05.040508Z",
     "shell.execute_reply.started": "2021-09-02T06:23:10.746008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict_test={'time_test':time_test,\r\n",
    "      'value_test_standard':value_test_standard,\r\n",
    "      'value_diff_test_standard':value_diff_test_standard,\r\n",
    "      'dis_diff_test_standard':dis_diff_test_standard,\r\n",
    "      'slope_test_standard':slope_test_standard,  \r\n",
    "      'sin_test_standard':sin_test_standard,\r\n",
    "      'cos_test_standard':cos_test_standard,\r\n",
    "      'add_test_standard':add_test_standard,\r\n",
    "      'multi_test_standard':multi_test_standard,\r\n",
    "      'new_test_alert_flag':new_test_alert_flag,\r\n",
    "      'test_alert_flag':data_test['alert_flag']}\r\n",
    "\r\n",
    "new_test_data=pd.DataFrame(dict_test)[1:955763].dropna()\r\n",
    "\r\n",
    "import featuretools as ft\r\n",
    "es_test=ft.EntitySet(id='feature_creation_test')\r\n",
    "es_test=es_test.entity_from_dataframe(entity_id='test_data',         \r\n",
    "                         dataframe=new_test_data[['value_test_standard','value_diff_test_standard','dis_diff_test_standard','slope_test_standard','sin_test_standard','cos_test_standard','add_test_standard','multi_test_standard']],\r\n",
    "                         index='index',\r\n",
    "                         make_index=True)\r\n",
    "trans_primitives_test=['add_numeric', 'multiply_numeric', 'subtract_numeric','divide_numeric']\r\n",
    "feature_matrix_test, feature_names_test = ft.dfs(entityset=es_test, target_entity='test_data',max_depth=100, trans_primitives=trans_primitives_test, max_features=112)\r\n",
    "feature_matrix_test['new_test_alert_flag']=new_test_alert_flag\r\n",
    "\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "pca_test=PCA(n_components='mle', svd_solver='full',random_state=10)\r\n",
    "data_pca_test=pca_test.fit_transform(feature_matrix_test)\r\n",
    "\r\n",
    "data_final=data_pca_test\r\n",
    "label_final=randomfor.predict(data_final)\r\n",
    "label_final=np.insert(label_final,0,[0])\r\n",
    "data_test['alert_flag']=label_final\r\n",
    "data_test.to_csv (path_or_buf=r'/data/jupyterhub/notebook/dickensyang_jupyter/Fault_Analysis/test_result/test')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T06:23:10.746036Z",
     "iopub.status.busy": "2021-09-02T06:23:10.745776Z",
     "iopub.status.idle": "2021-09-02T06:25:05.041174Z",
     "shell.execute_reply": "2021-09-02T06:25:05.040508Z",
     "shell.execute_reply.started": "2021-09-02T06:23:10.746008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\r\n",
    "import os\r\n",
    " \r\n",
    "path = '../development (4).csv'\r\n",
    " \r\n",
    "workspace = '../workspace'\r\n",
    " \r\n",
    "with open(path, 'r', newline='') as file:\r\n",
    "    csvreader = csv.reader(file)\r\n",
    "    a = next(csvreader)\r\n",
    "    print(a)\r\n",
    "    i = j = 0\r\n",
    "    for row in csvreader:\r\n",
    "        print(row)\r\n",
    "        print(f'i is {i}, j is {j}')\r\n",
    "        # 每128个就j加1， 然后就有一个新的文件名\r\n",
    "        if i % 128 == 0:\r\n",
    "            j += 1\r\n",
    "            print(f\"csv {j} 生成成功\")\r\n",
    "        #csv_path = os.path.join('../new_csv_file/', 'development (4)/' + str(j) + '.csv')\r\n",
    "        csv_path = os.path.join(workspace, 'part_{}.csv'.format(j))\r\n",
    "        # print('/'.join(path.split('/')[:-1]))\r\n",
    "        print(csv_path)\r\n",
    "        # 不存在此文件的时候，就创建\r\n",
    "        if not os.path.exists(os.path.dirname(csv_path)):\r\n",
    "            os.makedirs(os.path.dirname(csv_path))\r\n",
    "            with open(csv_path, 'w', newline='') as file:\r\n",
    "                csvwriter = csv.writer(file)\r\n",
    "                #csvwriter.writerow(['image_url'])\r\n",
    "                csvwriter.writerow(row)\r\n",
    "            i += 1\r\n",
    "        # 存在的时候就往里面添加\r\n",
    "        else:\r\n",
    "            with open(csv_path, 'a', newline='') as file:\r\n",
    "                csvwriter = csv.writer(file)\r\n",
    "                csvwriter.writerow(row)\r\n",
    "            i += \r\n"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T06:23:10.746036Z",
     "iopub.status.busy": "2021-09-02T06:23:10.745776Z",
     "iopub.status.idle": "2021-09-02T06:25:05.041174Z",
     "shell.execute_reply": "2021-09-02T06:25:05.040508Z",
     "shell.execute_reply.started": "2021-09-02T06:23:10.746008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "px.scatter(all_data[0:85221], x=all_data['time'][0:85221],y=all_data['value'][0:85221],color=all_data['alert_flag'][0:85221])"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-02T07:08:34.379869Z",
     "iopub.status.busy": "2021-09-02T07:08:34.379597Z",
     "iopub.status.idle": "2021-09-02T07:08:36.100941Z",
     "shell.execute_reply": "2021-09-02T07:08:36.100319Z",
     "shell.execute_reply.started": "2021-09-02T07:08:34.379839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "bf8098866661924d98d5d85c1d0c91bda70718c9a21bfc34d5e947ced32ae8fc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}